{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 1 — Configurações\n",
    "# =============================\n",
    "CSV_INPUT = 'resultado_tjma_v2.csv'  # ajuste para o seu caminho\n",
    "CSV_ROTULADO = 'resultado_tjma_rotulado.csv'\n",
    "CSV_MINIMAL = 'dataset_minimal_tjma.csv'\n",
    "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'  # BERTimbau\n",
    "MODEL_OUT_DIR = 'model_out_tjma'\n",
    "USE_TRANSFORMERS = True  # se quiser desativar, coloque False (usa TF-IDF baseline)\n",
    "SENSITIVE_COL = 'magistrado_genero'\n",
    "SENSITIVE_BIN = ['Feminino','Masculino']  # filtra apenas estes valores para fairness\n",
    "POSITIVE_CLASS = 'procedente'  # para métricas de paridade\n",
    "ADV_LAMBDA = 0.3  # peso da loss adversária\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 2 — Imports\n",
    "# =============================\n",
    "import re, os, json as pyjson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "\n",
    "# Tenta importar Fairlearn; se não houver, seguimos com cálculo manual\n",
    "try:\n",
    "    from fairlearn.metrics import MetricFrame, selection_rate, true_positive_rate, false_positive_rate\n",
    "    FAIRLEARN_OK = True\n",
    "except Exception as e:\n",
    "    FAIRLEARN_OK = False\n",
    "    print('Fairlearn não disponível, usarei métricas manuais. Erro:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de345f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 3 — Heurísticas de rotulagem por regex\n",
    "# =============================\n",
    "df = pd.read_csv(CSV_INPUT)\n",
    "len(df), df.head()\n",
    "\n",
    "# Regras heurísticas (iguais às usadas no pipeline anterior)\n",
    "PROCEDENTE = [\n",
    "    r'\b(julgo\\s+procedente)\b', r'\b(proced\\w+)\b', r'\b(dou\\s+provimento)\b', r'\b(acolho)\b', r'\b(condeno)\b',\n",
    "]\n",
    "IMPROCEDENTE = [\n",
    "    r'\b(julgo\\s+improcedente)\b', r'\b(improced\\w+)\b', r'\b(nego\\s+provimento)\b', r'\b(rejeito)\b', r'\b(desprovido|desprovimento)\b', r'\b(improcedentes\\s+os\\s+pedidos)\b',\n",
    "]\n",
    "NEUTRO = [\n",
    "    r'\b(julgo\\s+extinto|declaro\\s+extinto|extin\\w+\\s+do\\s+processo)\b',\n",
    "    r'\b(sem\\s+resolu\\w+\\s+do\\s+m\\w+rito|art\\.?\\s*485)\b',\n",
    "    r'\b(homologo\\s+acordo|homologa\\w+|homologo\\s+desist\\w+)\b',\n",
    "    r'\b(perda\\s+do\\s+objeto|car\\w+ncia\\s+de\\s+a\\w+\\w+o)\b',\n",
    "    r'\b(ilegitimidade|incompet\\w+ncia|litispend\\w+ncia)\b',\n",
    "    r'\b(indeferimento\\s+da\\s+peti\\w+\\s+inicial|indefiro\\s+a\\s+inicial|rejeito\\s+a\\s+inicial)\b',\n",
    "]\n",
    "MERITO = [r'\b(resolu\\w+\\s+do\\s+m\\w+rito)\b', r'\b(art\\.?\\s*487)\b']\n",
    "SEM_MERITO = [r'\b(sem\\s+resolu\\w+\\s+do\\s+m\\w+rito)\b', r'\b(art\\.?\\s*485)\b']\n",
    "\n",
    "\n",
    "def norm(s):\n",
    "    if not isinstance(s, str): return ''\n",
    "    s = s.strip()\n",
    "    return re.sub(r'\\s+', ' ', s)\n",
    "\n",
    "\n",
    "def find_first_match(text, patterns):\n",
    "    for rx in patterns:\n",
    "        if re.search(rx, text, flags=re.IGNORECASE):\n",
    "            return rx\n",
    "    return None\n",
    "\n",
    "\n",
    "def rotular_decisao(text):\n",
    "    t = norm(text)\n",
    "    ev = find_first_match(t, PROCEDENTE)\n",
    "    if ev:\n",
    "        tipo = 'merito' if find_first_match(t, MERITO) else ('sem_merito' if find_first_match(t, SEM_MERITO) else 'merito')\n",
    "        return ('procedente', ev, tipo, 0.95)\n",
    "    ev = find_first_match(t, IMPROCEDENTE)\n",
    "    if ev:\n",
    "        tipo = 'merito' if find_first_match(t, MERITO) else ('sem_merito' if find_first_match(t, SEM_MERITO) else 'merito')\n",
    "        return ('improcedente', ev, tipo, 0.95)\n",
    "    ev = find_first_match(t, NEUTRO)\n",
    "    if ev:\n",
    "        return ('neutro', ev, 'sem_merito', 0.90)\n",
    "    if find_first_match(t, MERITO):\n",
    "        return ('neutro', None, 'merito', 0.60)\n",
    "    if find_first_match(t, SEM_MERITO):\n",
    "        return ('neutro', None, 'sem_merito', 0.70)\n",
    "    return ('neutro', None, None, 0.50)\n",
    "\n",
    "out = df.copy()\n",
    "res = out['decisao'].apply(rotular_decisao)\n",
    "out['sentimento'] = res.apply(lambda x: x[0])\n",
    "out['evidencia'] = res.apply(lambda x: x[1])\n",
    "out['tipo_resultado'] = res.apply(lambda x: x[2])\n",
    "out['confianca'] = res.apply(lambda x: x[3])\n",
    "print('Distribuição de rótulos:')\n",
    "print(out['sentimento'].value_counts())\n",
    "\n",
    "out.to_csv(CSV_ROTULADO, index=False)\n",
    "minimal = out[['decisao','sentimento',SENSITIVE_COL]].rename(columns={'decisao':'text','sentimento':'label'})\n",
    "minimal['text'] = minimal['text'].fillna('').astype(str).str.strip()\n",
    "# CORREÇÃO: operador '>' (antes estava &gt;)\n",
    "minimal = minimal[minimal['text'].str.len() > 3]\n",
    "minimal.to_csv(CSV_MINIMAL, index=False)\n",
    "CSV_ROTULADO, CSV_MINIMAL, len(minimal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e816b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 4 — Mapeamento e splits\n",
    "# =============================\n",
    "label2id = {'procedente':0, 'improcedente':1, 'neutro':2}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "minimal['label_id'] = minimal['label'].map(label2id)\n",
    "minimal = minimal.dropna(subset=['label_id'])\n",
    "\n",
    "# filtra apenas F/M para fairness\n",
    "assert len(SENSITIVE_BIN) == 2, \"SENSITIVE_BIN deve ter exatamente dois grupos.\"\n",
    "minimal = minimal[minimal[SENSITIVE_COL].isin(SENSITIVE_BIN)]\n",
    "\n",
    "# Split train/test estratificado\n",
    "train_df, test_df = train_test_split(minimal, test_size=0.2, random_state=42, stratify=minimal['label_id'])\n",
    "# Split validação a partir do treino (correção: não usar 'test' como validação no Trainer)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label_id'])\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8713ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 5 — Treino base (Transformers ou TF-IDF)\n",
    "# =============================\n",
    "pred_base = None\n",
    "if USE_TRANSFORMERS:\n",
    "    from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments)\n",
    "    from transformers import DataCollatorWithPadding\n",
    "    from datasets import Dataset, DatasetDict\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df[['text','label_id']].rename(columns={'label_id':'labels'}))\n",
    "    val_ds   = Dataset.from_pandas(val_df[['text','label_id']].rename(columns={'label_id':'labels'}))\n",
    "    test_ds  = Dataset.from_pandas(test_df[['text','label_id']].rename(columns={'label_id':'labels'}))\n",
    "    ds = DatasetDict({'train':train_ds, 'val':val_ds, 'test':test_ds})\n",
    "\n",
    "    def tok(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, max_length=512)\n",
    "\n",
    "    ds_tok = ds.map(tok, batched=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    import numpy as np\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "            'recall_macro': recall_score(labels, preds, average='macro', zero_division=0),\n",
    "            'f1_macro': f1_score(labels, preds, average='macro', zero_division=0),\n",
    "        }\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=MODEL_OUT_DIR,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        report_to='none',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1_macro',\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    from transformers import EarlyStoppingCallback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_tok['train'],\n",
    "        eval_dataset=ds_tok['val'],  # CORREÇÃO: usar validação, não 'test'\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate(ds_tok['val'])\n",
    "    preds = trainer.predict(ds_tok['test'])\n",
    "    y_true = np.array(preds.label_ids)\n",
    "    y_pred = np.argmax(preds.predictions, axis=-1)\n",
    "else:\n",
    "    # Fallback rápido: TF-IDF + LogisticRegression\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    vec = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
    "    Xtr = vec.fit_transform(train_df['text'])\n",
    "    Xte = vec.transform(test_df['text'])\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(Xtr, train_df['label_id'])\n",
    "    y_pred = clf.predict(Xte)\n",
    "    y_true = np.array(test_df['label_id'])\n",
    "    eval_metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "    }\n",
    "\n",
    "# Verificação de alinhamento\n",
    "assert len(test_df) == len(y_true) == len(y_pred), \"Tamanhos de teste e vetores y_true/y_pred não batem.\"\n",
    "\n",
    "eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05284505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 6 — Fairness (modelo base)\n",
    "# =============================\n",
    "# Junta rótulos e grupos no teste\n",
    "assert all(g in test_df[SENSITIVE_COL].unique() for g in SENSITIVE_BIN), \"Nem todos os grupos de SENSITIVE_BIN estão presentes no teste.\"\n",
    "\n",
    "test_eval = test_df.copy()\n",
    "test_eval['y_true'] = y_true\n",
    "test_eval['y_pred'] = y_pred\n",
    "\n",
    "# Define 'positivo' como classe 0 (procedente)\n",
    "POS_ID = label2id[POSITIVE_CLASS]\n",
    "\n",
    "def rate_positive(y):\n",
    "    return np.mean(np.array(y)==POS_ID)\n",
    "\n",
    "\n",
    "def group_rates(df_in, col=SENSITIVE_COL):\n",
    "    res = {}\n",
    "    for g in SENSITIVE_BIN:\n",
    "        dfg = df_in[df_in[col]==g]\n",
    "        res[g] = {\n",
    "            'selection_rate_true': rate_positive(dfg['y_true']),\n",
    "            'selection_rate_pred': rate_positive(dfg['y_pred']),\n",
    "        }\n",
    "        # TPR/FPR por grupo no modelo (CORREÇÃO: operadores &)\n",
    "        yt = (dfg['y_true'].values==POS_ID).astype(int)\n",
    "        yp = (dfg['y_pred'].values==POS_ID).astype(int)\n",
    "        TPR = (np.sum((yp==1) & (yt==1)) / max(np.sum(yt==1), 1))\n",
    "        FPR = (np.sum((yp==1) & (yt==0)) / max(np.sum(yt==0), 1))\n",
    "        res[g]['TPR'] = TPR\n",
    "        res[g]['FPR'] = FPR\n",
    "    return res\n",
    "\n",
    "rates = group_rates(test_eval)\n",
    "\n",
    "# Diferenças de disparidade (diferença absoluta entre grupos)\n",
    "def disparity_diffs(rates):\n",
    "    g0, g1 = SENSITIVE_BIN\n",
    "    diffs = {\n",
    "        'demographic_parity_true_diff': abs(rates[g0]['selection_rate_true'] - rates[g1]['selection_rate_true']),\n",
    "        'demographic_parity_pred_diff': abs(rates[g0]['selection_rate_pred'] - rates[g1]['selection_rate_pred']),\n",
    "        'equal_opportunity_diff': abs(rates[g0]['TPR'] - rates[g1]['TPR']),\n",
    "        'equalized_odds_FPR_diff': abs(rates[g0]['FPR'] - rates[g1]['FPR']),\n",
    "    }\n",
    "    return diffs\n",
    "\n",
    "diffs_base = disparity_diffs(rates)\n",
    "diffs_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bebdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 7 — Visualização (modelo base)\n",
    "# =============================\n",
    "g0, g1 = SENSITIVE_BIN\n",
    "labels_plot = ['TPR','FPR']\n",
    "vals0 = [rates[g0]['TPR'], rates[g0]['FPR']]\n",
    "vals1 = [rates[g1]['TPR'], rates[g1]['FPR']]\n",
    "x = np.arange(len(labels_plot))\n",
    "w = 0.35\n",
    "plt.bar(x-w/2, vals0, width=w, label=g0)\n",
    "plt.bar(x+w/2, vals1, width=w, label=g1)\n",
    "plt.xticks(x, labels_plot)\n",
    "plt.ylim(0,1)\n",
    "plt.title('TPR/FPR por gênero (modelo base)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 8 — GRL e AdvModel\n",
    "# =============================\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "\n",
    "class GRL(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "class AdvModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=3):\n",
    "        super().__init__()\n",
    "        from transformers import AutoModel\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden, num_labels)\n",
    "        self.adv_head = nn.Linear(hidden, 2)  # gênero binário\n",
    "    def forward(self, **inputs):\n",
    "        out = self.encoder(**inputs)\n",
    "        h = out.last_hidden_state[:,0,:]  # CLS\n",
    "        logits_cls = self.classifier(h)\n",
    "        return logits_cls, h\n",
    "    def adv(self, h, lambda_):\n",
    "        h_grl = GRL.apply(h, lambda_)\n",
    "        logits_adv = self.adv_head(h_grl)\n",
    "        return logits_adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc82c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 9 — Definição de TextDataset (correção)\n",
    "# =============================\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer):\n",
    "        self.texts = df['text'].astype(str).tolist()\n",
    "        self.labels = df['label_id'].astype(int).tolist()\n",
    "        # mapeia gênero para {0,1} conforme ordem em SENSITIVE_BIN\n",
    "        mapping = {SENSITIVE_BIN[0]:0, SENSITIVE_BIN[1]:1}\n",
    "        self.gender = df[SENSITIVE_COL].map(mapping).astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',  # CORREÇÃO: garantir padding uniforme para DataLoader\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'gender': torch.tensor(self.gender[idx], dtype=torch.long),\n",
    "        }\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 10 — Treino adversarial (correções aplicadas)\n",
    "# =============================\n",
    "if USE_TRANSFORMERS:\n",
    "    from transformers import AutoTokenizer\n",
    "    tok2 = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    tr_ds = TextDataset(train_df[['text','label_id',SENSITIVE_COL]], tok2)\n",
    "    te_ds = TextDataset(test_df[['text','label_id',SENSITIVE_COL]], tok2)\n",
    "\n",
    "    tr_dl = DataLoader(tr_ds, batch_size=16, shuffle=True)\n",
    "    te_dl = DataLoader(te_ds, batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    adv_model = AdvModel(MODEL_NAME).to(device)\n",
    "    opt = torch.optim.AdamW(adv_model.parameters(), lr=2e-5)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Treino curto (1 época) só para demonstrar\n",
    "    adv_model.train()\n",
    "    for epoch in range(1):\n",
    "        for batch in tr_dl:\n",
    "            inputs = {k:batch[k].to(device) for k in ['input_ids','attention_mask']}\n",
    "            y = batch['labels'].to(device)\n",
    "            g = batch['gender'].to(device)\n",
    "\n",
    "            logits_cls, h = adv_model(**inputs)\n",
    "            loss_cls = ce(logits_cls, y)\n",
    "\n",
    "            # CORREÇÃO: usar GRL com lambda=1.0 e ponderar loss adversária por ADV_LAMBDA (sem sinal negativo)\n",
    "            logits_adv = adv_model.adv(h, 1.0)\n",
    "            loss_adv = ce(logits_adv, g)\n",
    "            loss = loss_cls + ADV_LAMBDA * loss_adv\n",
    "\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # Avaliação\n",
    "    adv_model.eval()\n",
    "    y_true2, y_pred2, groups2 = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in te_dl:\n",
    "            inputs = {k:batch[k].to(device) for k in ['input_ids','attention_mask']}\n",
    "            logits_cls, _ = adv_model(**inputs)\n",
    "            pred = torch.argmax(logits_cls, dim=-1).cpu().numpy()\n",
    "            y_pred2.extend(list(pred))\n",
    "            y_true2.extend(list(batch['labels'].numpy()))\n",
    "            groups2.extend(list(batch['gender'].numpy()))\n",
    "\n",
    "    y_true2 = np.array(y_true2); y_pred2 = np.array(y_pred2); groups2 = np.array(groups2)\n",
    "\n",
    "    # Verificação de alinhamento\n",
    "    assert len(test_df) == len(y_true2) == len(y_pred2), \"Tamanhos do teste e vetores y_true2/y_pred2 não batem.\"\n",
    "\n",
    "    # Monta DF para métricas\n",
    "    test_eval_adv = test_df.copy()\n",
    "    test_eval_adv['y_true'] = y_true2\n",
    "    test_eval_adv['y_pred'] = y_pred2\n",
    "    test_eval_adv['bin_gender'] = np.where(test_eval_adv[SENSITIVE_COL]=='Feminino','Feminino','Masculino')\n",
    "\n",
    "    # Calcula disparidades (CORREÇÃO: operadores '&')\n",
    "    def compute_rates_df(df_in):\n",
    "        res = {}\n",
    "        for g in SENSITIVE_BIN:\n",
    "            dfg = df_in[df_in['bin_gender']==g]\n",
    "            sel_pred = np.mean(dfg['y_pred'].values==POS_ID)\n",
    "            yt = (dfg['y_true'].values==POS_ID).astype(int); yp = (dfg['y_pred'].values==POS_ID).astype(int)\n",
    "            TPR = (np.sum((yp==1) & (yt==1)) / max(np.sum(yt==1), 1))\n",
    "            FPR = (np.sum((yp==1) & (yt==0)) / max(np.sum(yt==0), 1))\n",
    "            res[g] = {'selection_rate_pred': sel_pred, 'TPR': TPR, 'FPR': FPR}\n",
    "        return res\n",
    "\n",
    "    rates_adv = compute_rates_df(test_eval_adv)\n",
    "    diffs_adv = {\n",
    "        'demographic_parity_pred_diff': abs(rates_adv[SENSITIVE_BIN[0]]['selection_rate_pred'] - rates_adv[SENSITIVE_BIN[1]]['selection_rate_pred']),\n",
    "        'equal_opportunity_diff': abs(rates_adv[SENSITIVE_BIN[0]]['TPR'] - rates_adv[SENSITIVE_BIN[1]]['TPR']),\n",
    "        'equalized_odds_FPR_diff': abs(rates_adv[SENSITIVE_BIN[0]]['FPR'] - rates_adv[SENSITIVE_BIN[1]]['FPR']),\n",
    "    }\n",
    "    rates_adv, diffs_adv\n",
    "else:\n",
    "    print('Adversarial requer Transformers; ative USE_TRANSFORMERS=True e instale dependências.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3832a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 11 — Comparação Base vs Adversarial\n",
    "# =============================\n",
    "if USE_TRANSFORMERS:\n",
    "    print('Base:', diffs_base)\n",
    "    print('Adv :', diffs_adv)\n",
    "    # Barras de comparação (paridade predita)\n",
    "    dp_base = diffs_base['demographic_parity_pred_diff']\n",
    "    eo_base = diffs_base['equal_opportunity_diff']\n",
    "    eod_base = diffs_base['equalized_odds_FPR_diff']\n",
    "    dp_adv = diffs_adv['demographic_parity_pred_diff']\n",
    "    eo_adv = diffs_adv['equal_opportunity_diff']\n",
    "    eod_adv = diffs_adv['equalized_odds_FPR_diff']\n",
    "    lbls = ['Parity diff','EO diff (TPR)','EOdds diff (FPR)']\n",
    "    base_vals = [dp_base, eo_base, eod_base]\n",
    "    adv_vals = [dp_adv, eo_adv, eod_adv]\n",
    "    x = np.arange(len(lbls)); w=0.35\n",
    "    plt.bar(x-w/2, base_vals, width=w, label='Base')\n",
    "    plt.bar(x+w/2, adv_vals, width=w, label='Adversarial')\n",
    "    plt.xticks(x, lbls); plt.title('Disparidades por gênero — Base vs. Adversarial')\n",
    "    plt.legend(); plt.show()\n",
    "else:\n",
    "    print('Sem Transformers, comparação adversarial não foi executada.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# CÉLULA 12 — Relatório JSON\n",
    "# =============================\n",
    "report = {\n",
    "  'base': diffs_base,\n",
    "}\n",
    "if USE_TRANSFORMERS:\n",
    "    report['adversarial'] = diffs_adv\n",
    "with open('fairness_report.json','w',encoding='utf-8') as f:\n",
    "    pyjson.dump(report, f, ensure_ascii=False, indent=2)\n",
    "'fairness_report.json'\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
